{
    "short": "Predict presence of precipitation based on text of tweets",
    "long": "# Obtain weather related information from Tweets\\n\\nTo improve predictions of the weather, we would like to harvest weather related information from social media sources. In practice, we aim to predict the presence of rain based on tweets. \\nWe employ transformer based deep learning models like [DeBerta](https://arxiv.org/abs/2006.03654) with a classification head. \\nDatasets\\nWe use historical tweets from the years 2017-2020 that include keywords related to the presence of rain, e.g. rain, sunny or drizzle. Location needs to be attached to these tweets such that we can assign weather data to them. Locations can be assigned to Tweets in two ways. The user may opt in to be tracked by Twitter such that GPS data is assigned to every tweet of the location of the user. Alternatively, users can tag their tweets from a curated list of locations by Twitter. We use Tweets with locations from either method.  However, we exclude tweets that are tagged with a location with surface area greater than 100 km2, which corresponds to the spatial resolution of our weather data.  Here, we focus on tweets in the English language that originate from any location within the United Kingdom. This allows us to resort to the most popular pretrained models in natural language processing (NLP), which are usually trained in English. \\nTo improve results, texts are preprocessed to match vocabulary of the model and the input as well as focusing the model's attention on the relevant parts of the text. We remove hyperlinks and hashtags (unless they represent a keyword) and rephrase colloquial terms. If emojis are directly related to precipitation, e.g. umbrella, rain cloud, these emojis are converted to text. Remaining emojis are removed from the text. For training, we only retain texts that still contain at least one keyword after preprocessing.\\nMetrics and loss function\\nWe use cross entropy as our loss function. The proficiency of the model is evaluated via the f1 score of the minority class (not raining). The f1 score is the harmonic mean of the precision and recall.\\nOverview models\\nGenerally, our problem can be phrased as a text classification task, where deep learning models based on the  transformer architecture (Vaswani et al. 2017) achieve state of the art results (e.g., Yang et al. 2019). Transformers use self-attention to attribute varying relevance to different parts of the text. Based on transformers the BERT model was developed (Devlin et al. 2019), which includes crucial pre-training steps to familiarise the model with relevant vocabulary and semantics. In addition, the DeBERTa model (He et al. 2021a) adds additional pre-training steps and disentangles how positional and text information is stored in the model to improve performance. For this experiment we rely on the most recent version of the model, which is DeBERTaV3 (He et al. 2021b).\\nWe use the default version of DeBERTaV3_small as described in He et al. 2021b with an adopted head for text classification. For this, we add a dropout layer with user-specified dropout rate and a pooling layer that passes the embedding of the special initial token, which supposedly comprises the meaning of the whole Tweet, to the loss function.\\n"
}
